{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'transformers'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtransformers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m pipeline\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mmath\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'transformers'"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "import math\n",
    "import torch\n",
    "from transformers import AutoTokenizer, GPT2LMHeadModel, EsmForProteinFolding\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import argparse\n",
    "import yaml\n",
    "import os\n",
    "import shutil\n",
    "import gc\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'numpy' has no attribute 'ndarray'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Use a pipeline as a high-level helper\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtransformers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m pipeline\n\u001b[0;32m      4\u001b[0m pipe \u001b[38;5;241m=\u001b[39m pipeline(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtext-generation\u001b[39m\u001b[38;5;124m\"\u001b[39m, model\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnferruz/ProtGPT2\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32md:\\Anaconda3\\envs\\BIA\\lib\\site-packages\\transformers\\__init__.py:26\u001b[0m\n\u001b[0;32m     23\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtyping\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m TYPE_CHECKING\n\u001b[0;32m     25\u001b[0m \u001b[38;5;66;03m# Check the dependencies satisfy the minimal versions required.\u001b[39;00m\n\u001b[1;32m---> 26\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m dependency_versions_check\n\u001b[0;32m     27\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[0;32m     28\u001b[0m     OptionalDependencyNotAvailable,\n\u001b[0;32m     29\u001b[0m     _LazyModule,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     48\u001b[0m     logging,\n\u001b[0;32m     49\u001b[0m )\n\u001b[0;32m     52\u001b[0m logger \u001b[38;5;241m=\u001b[39m logging\u001b[38;5;241m.\u001b[39mget_logger(\u001b[38;5;18m__name__\u001b[39m)  \u001b[38;5;66;03m# pylint: disable=invalid-name\u001b[39;00m\n",
      "File \u001b[1;32md:\\Anaconda3\\envs\\BIA\\lib\\site-packages\\transformers\\dependency_versions_check.py:16\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Copyright 2020 The HuggingFace Team. All rights reserved.\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;66;03m# Licensed under the Apache License, Version 2.0 (the \"License\");\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[38;5;66;03m# See the License for the specific language governing permissions and\u001b[39;00m\n\u001b[0;32m     13\u001b[0m \u001b[38;5;66;03m# limitations under the License.\u001b[39;00m\n\u001b[0;32m     15\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdependency_versions_table\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m deps\n\u001b[1;32m---> 16\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mversions\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m require_version, require_version_core\n\u001b[0;32m     19\u001b[0m \u001b[38;5;66;03m# define which module versions we always want to check at run time\u001b[39;00m\n\u001b[0;32m     20\u001b[0m \u001b[38;5;66;03m# (usually the ones defined in `install_requires` in setup.py)\u001b[39;00m\n\u001b[0;32m     21\u001b[0m \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[0;32m     22\u001b[0m \u001b[38;5;66;03m# order specific notes:\u001b[39;00m\n\u001b[0;32m     23\u001b[0m \u001b[38;5;66;03m# - tqdm must be checked before tokenizers\u001b[39;00m\n\u001b[0;32m     25\u001b[0m pkgs_to_check_at_runtime \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m     26\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpython\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m     27\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtqdm\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     37\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpyyaml\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m     38\u001b[0m ]\n",
      "File \u001b[1;32md:\\Anaconda3\\envs\\BIA\\lib\\site-packages\\transformers\\utils\\__init__.py:25\u001b[0m\n\u001b[0;32m     23\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m __version__\n\u001b[0;32m     24\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbackbone_utils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m BackboneConfigMixin, BackboneMixin\n\u001b[1;32m---> 25\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mchat_template_utils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m DocstringParsingException, TypeHintParsingException, get_json_schema\n\u001b[0;32m     26\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mconstants\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m IMAGENET_DEFAULT_MEAN, IMAGENET_DEFAULT_STD, IMAGENET_STANDARD_MEAN, IMAGENET_STANDARD_STD\n\u001b[0;32m     27\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdoc\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[0;32m     28\u001b[0m     add_code_sample_docstrings,\n\u001b[0;32m     29\u001b[0m     add_end_docstrings,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     33\u001b[0m     replace_return_docstrings,\n\u001b[0;32m     34\u001b[0m )\n",
      "File \u001b[1;32md:\\Anaconda3\\envs\\BIA\\lib\\site-packages\\transformers\\utils\\chat_template_utils.py:40\u001b[0m\n\u001b[0;32m     37\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mPIL\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mImage\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Image\n\u001b[0;32m     39\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_torch_available():\n\u001b[1;32m---> 40\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Tensor\n\u001b[0;32m     43\u001b[0m BASIC_TYPES \u001b[38;5;241m=\u001b[39m (\u001b[38;5;28mint\u001b[39m, \u001b[38;5;28mfloat\u001b[39m, \u001b[38;5;28mstr\u001b[39m, \u001b[38;5;28mbool\u001b[39m, Any, \u001b[38;5;28mtype\u001b[39m(\u001b[38;5;28;01mNone\u001b[39;00m), \u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m)\n\u001b[0;32m     44\u001b[0m \u001b[38;5;66;03m# Extracts the initial segment of the docstring, containing the function description\u001b[39;00m\n",
      "File \u001b[1;32md:\\Anaconda3\\envs\\BIA\\lib\\site-packages\\torch\\__init__.py:2016\u001b[0m\n\u001b[0;32m   2009\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_compile\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m _disable_dynamo  \u001b[38;5;66;03m# usort: skip\u001b[39;00m\n\u001b[0;32m   2011\u001b[0m \u001b[38;5;66;03m################################################################################\u001b[39;00m\n\u001b[0;32m   2012\u001b[0m \u001b[38;5;66;03m# Import interface functions defined in Python\u001b[39;00m\n\u001b[0;32m   2013\u001b[0m \u001b[38;5;66;03m################################################################################\u001b[39;00m\n\u001b[0;32m   2014\u001b[0m \n\u001b[0;32m   2015\u001b[0m \u001b[38;5;66;03m# needs to be after the above ATen bindings so we can overwrite from Python side\u001b[39;00m\n\u001b[1;32m-> 2016\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m _VF \u001b[38;5;28;01mas\u001b[39;00m _VF, functional \u001b[38;5;28;01mas\u001b[39;00m functional  \u001b[38;5;66;03m# usort: skip\u001b[39;00m\n\u001b[0;32m   2017\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfunctional\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m  \u001b[38;5;66;03m# usort: skip # noqa: F403\u001b[39;00m\n\u001b[0;32m   2019\u001b[0m \u001b[38;5;66;03m################################################################################\u001b[39;00m\n\u001b[0;32m   2020\u001b[0m \u001b[38;5;66;03m# Remove unnecessary members\u001b[39;00m\n\u001b[0;32m   2021\u001b[0m \u001b[38;5;66;03m################################################################################\u001b[39;00m\n",
      "File \u001b[1;32md:\\Anaconda3\\envs\\BIA\\lib\\site-packages\\torch\\functional.py:7\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtyping\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Any, List, Optional, Sequence, Tuple, TYPE_CHECKING, Union\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\n\u001b[1;32m----> 7\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mnn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfunctional\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mF\u001b[39;00m\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m _VF, Tensor\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_C\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m _add_docstr\n",
      "File \u001b[1;32md:\\Anaconda3\\envs\\BIA\\lib\\site-packages\\torch\\nn\\__init__.py:8\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# mypy: allow-untyped-defs\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mnn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mparameter\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (  \u001b[38;5;66;03m# usort: skip\u001b[39;00m\n\u001b[0;32m      3\u001b[0m     Buffer \u001b[38;5;28;01mas\u001b[39;00m Buffer,\n\u001b[0;32m      4\u001b[0m     Parameter \u001b[38;5;28;01mas\u001b[39;00m Parameter,\n\u001b[0;32m      5\u001b[0m     UninitializedBuffer \u001b[38;5;28;01mas\u001b[39;00m UninitializedBuffer,\n\u001b[0;32m      6\u001b[0m     UninitializedParameter \u001b[38;5;28;01mas\u001b[39;00m UninitializedParameter,\n\u001b[0;32m      7\u001b[0m )\n\u001b[1;32m----> 8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mnn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodules\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m  \u001b[38;5;66;03m# usort: skip # noqa: F403\u001b[39;00m\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mnn\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[0;32m     10\u001b[0m     attention \u001b[38;5;28;01mas\u001b[39;00m attention,\n\u001b[0;32m     11\u001b[0m     functional \u001b[38;5;28;01mas\u001b[39;00m functional,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     16\u001b[0m     utils \u001b[38;5;28;01mas\u001b[39;00m utils,\n\u001b[0;32m     17\u001b[0m )\n\u001b[0;32m     18\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mnn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mparallel\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m DataParallel \u001b[38;5;28;01mas\u001b[39;00m DataParallel\n",
      "File \u001b[1;32md:\\Anaconda3\\envs\\BIA\\lib\\site-packages\\torch\\nn\\modules\\__init__.py:1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodule\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Module  \u001b[38;5;66;03m# usort: skip\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlinear\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Bilinear, Identity, LazyLinear, Linear  \u001b[38;5;66;03m# usort: skip\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mactivation\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[0;32m      4\u001b[0m     CELU,\n\u001b[0;32m      5\u001b[0m     ELU,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     32\u001b[0m     Threshold,\n\u001b[0;32m     33\u001b[0m )\n",
      "File \u001b[1;32md:\\Anaconda3\\envs\\BIA\\lib\\site-packages\\torch\\nn\\modules\\module.py:29\u001b[0m\n\u001b[0;32m     27\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_prims_common\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m DeviceLikeType\n\u001b[0;32m     28\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mnn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mparameter\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Buffer, Parameter\n\u001b[1;32m---> 29\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_python_dispatch\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m is_traceable_wrapper_subclass\n\u001b[0;32m     30\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mhooks\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m BackwardHook, RemovableHandle\n\u001b[0;32m     33\u001b[0m __all__ \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m     34\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mregister_module_forward_pre_hook\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m     35\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mregister_module_forward_hook\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     42\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mModule\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m     43\u001b[0m ]\n",
      "File \u001b[1;32md:\\Anaconda3\\envs\\BIA\\lib\\site-packages\\torch\\utils\\__init__.py:8\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mweakref\u001b[39;00m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\n\u001b[1;32m----> 8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[0;32m      9\u001b[0m     backcompat \u001b[38;5;28;01mas\u001b[39;00m backcompat,\n\u001b[0;32m     10\u001b[0m     collect_env \u001b[38;5;28;01mas\u001b[39;00m collect_env,\n\u001b[0;32m     11\u001b[0m     data \u001b[38;5;28;01mas\u001b[39;00m data,\n\u001b[0;32m     12\u001b[0m     deterministic \u001b[38;5;28;01mas\u001b[39;00m deterministic,\n\u001b[0;32m     13\u001b[0m     hooks \u001b[38;5;28;01mas\u001b[39;00m hooks,\n\u001b[0;32m     14\u001b[0m )\n\u001b[0;32m     15\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbackend_registration\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[0;32m     16\u001b[0m     generate_methods_for_privateuse1_backend,\n\u001b[0;32m     17\u001b[0m     rename_privateuse1_backend,\n\u001b[0;32m     18\u001b[0m )\n\u001b[0;32m     19\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcpp_backtrace\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m get_cpp_backtrace\n",
      "File \u001b[1;32md:\\Anaconda3\\envs\\BIA\\lib\\site-packages\\torch\\utils\\data\\__init__.py:1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdata\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdataloader\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[0;32m      2\u001b[0m     _DatasetKind,\n\u001b[0;32m      3\u001b[0m     DataLoader,\n\u001b[0;32m      4\u001b[0m     default_collate,\n\u001b[0;32m      5\u001b[0m     default_convert,\n\u001b[0;32m      6\u001b[0m     get_worker_info,\n\u001b[0;32m      7\u001b[0m )\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdata\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdatapipes\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_decorator\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[0;32m      9\u001b[0m     argument_validation,\n\u001b[0;32m     10\u001b[0m     functional_datapipe,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     14\u001b[0m     runtime_validation_disabled,\n\u001b[0;32m     15\u001b[0m )\n\u001b[0;32m     16\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdata\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdatapipes\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdatapipe\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[0;32m     17\u001b[0m     DataChunk,\n\u001b[0;32m     18\u001b[0m     DFIterDataPipe,\n\u001b[0;32m     19\u001b[0m     IterDataPipe,\n\u001b[0;32m     20\u001b[0m     MapDataPipe,\n\u001b[0;32m     21\u001b[0m )\n",
      "File \u001b[1;32md:\\Anaconda3\\envs\\BIA\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:21\u001b[0m\n\u001b[0;32m     19\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\n\u001b[0;32m     20\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdistributed\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mdist\u001b[39;00m\n\u001b[1;32m---> 21\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdata\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mgraph_settings\u001b[39;00m\n\u001b[0;32m     22\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_utils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ExceptionWrapper\n\u001b[0;32m     23\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdata\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m _utils\n",
      "File \u001b[1;32md:\\Anaconda3\\envs\\BIA\\lib\\site-packages\\torch\\utils\\data\\graph_settings.py:8\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtyping_extensions\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m deprecated\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\n\u001b[1;32m----> 8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdata\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdatapipes\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01miter\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msharding\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[0;32m      9\u001b[0m     _ShardingIterDataPipe,\n\u001b[0;32m     10\u001b[0m     SHARDING_PRIORITIES,\n\u001b[0;32m     11\u001b[0m )\n\u001b[0;32m     12\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdata\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mgraph\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m DataPipe, DataPipeGraph, traverse_dps\n\u001b[0;32m     15\u001b[0m __all__ \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m     16\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mapply_random_seed\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m     17\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mapply_sharding\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     20\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mget_all_graph_pipes\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m     21\u001b[0m ]\n",
      "File \u001b[1;32md:\\Anaconda3\\envs\\BIA\\lib\\site-packages\\torch\\utils\\data\\datapipes\\__init__.py:1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdata\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdatapipes\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m dataframe \u001b[38;5;28;01mas\u001b[39;00m dataframe, \u001b[38;5;28miter\u001b[39m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;28miter\u001b[39m, \u001b[38;5;28mmap\u001b[39m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;28mmap\u001b[39m\n",
      "File \u001b[1;32md:\\Anaconda3\\envs\\BIA\\lib\\site-packages\\torch\\utils\\data\\datapipes\\iter\\__init__.py:1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdata\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdatapipes\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01miter\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcallable\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[0;32m      2\u001b[0m     CollatorIterDataPipe \u001b[38;5;28;01mas\u001b[39;00m Collator,\n\u001b[0;32m      3\u001b[0m     MapperIterDataPipe \u001b[38;5;28;01mas\u001b[39;00m Mapper,\n\u001b[0;32m      4\u001b[0m )\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdata\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdatapipes\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01miter\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcombinatorics\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[0;32m      6\u001b[0m     SamplerIterDataPipe \u001b[38;5;28;01mas\u001b[39;00m Sampler,\n\u001b[0;32m      7\u001b[0m     ShufflerIterDataPipe \u001b[38;5;28;01mas\u001b[39;00m Shuffler,\n\u001b[0;32m      8\u001b[0m )\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdata\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdatapipes\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01miter\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcombining\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[0;32m     10\u001b[0m     ConcaterIterDataPipe \u001b[38;5;28;01mas\u001b[39;00m Concater,\n\u001b[0;32m     11\u001b[0m     DemultiplexerIterDataPipe \u001b[38;5;28;01mas\u001b[39;00m Demultiplexer,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     14\u001b[0m     ZipperIterDataPipe \u001b[38;5;28;01mas\u001b[39;00m Zipper,\n\u001b[0;32m     15\u001b[0m )\n",
      "File \u001b[1;32md:\\Anaconda3\\envs\\BIA\\lib\\site-packages\\torch\\utils\\data\\datapipes\\iter\\callable.py:6\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mcollections\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m namedtuple\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtyping\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Any, Callable, Dict, Iterator, List, Optional, Sized, TypeVar, Union\n\u001b[1;32m----> 6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdata\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_utils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcollate\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m default_collate\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdata\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdatapipes\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_decorator\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m functional_datapipe\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdata\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdatapipes\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdataframe\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m dataframe_wrapper \u001b[38;5;28;01mas\u001b[39;00m df_wrapper\n",
      "File \u001b[1;32md:\\Anaconda3\\envs\\BIA\\lib\\site-packages\\torch\\utils\\data\\_utils\\__init__.py:54\u001b[0m\n\u001b[0;32m     48\u001b[0m     python_exit_status \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m     51\u001b[0m atexit\u001b[38;5;241m.\u001b[39mregister(_set_python_exit_flag)\n\u001b[1;32m---> 54\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m collate, fetch, pin_memory, signal_handling, worker\n",
      "File \u001b[1;32md:\\Anaconda3\\envs\\BIA\\lib\\site-packages\\torch\\utils\\data\\_utils\\collate.py:327\u001b[0m\n\u001b[0;32m    324\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[0;32m    326\u001b[0m \u001b[38;5;66;03m# For both ndarray and memmap (subclass of ndarray)\u001b[39;00m\n\u001b[1;32m--> 327\u001b[0m default_collate_fn_map[\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mndarray\u001b[49m] \u001b[38;5;241m=\u001b[39m collate_numpy_array_fn\n\u001b[0;32m    328\u001b[0m \u001b[38;5;66;03m# See scalars hierarchy: https://numpy.org/doc/stable/reference/arrays.scalars.html\u001b[39;00m\n\u001b[0;32m    329\u001b[0m \u001b[38;5;66;03m# Skip string scalars\u001b[39;00m\n\u001b[0;32m    330\u001b[0m default_collate_fn_map[(np\u001b[38;5;241m.\u001b[39mbool_, np\u001b[38;5;241m.\u001b[39mnumber, np\u001b[38;5;241m.\u001b[39mobject_)] \u001b[38;5;241m=\u001b[39m collate_numpy_scalar_fn\n",
      "\u001b[1;31mAttributeError\u001b[0m: module 'numpy' has no attribute 'ndarray'"
     ]
    }
   ],
   "source": [
    "# Use a pipeline as a high-level helper\n",
    "from transformers import pipeline\n",
    "\n",
    "pipe = pipeline(\"text-generation\", model=\"nferruz/ProtGPT2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_or_replace_directory(directory_path):\n",
    "    \"\"\"\n",
    "    Check if a directory exists. If it exists, delete it and create a new one.\n",
    "    If it doesn't exist, create the directory.\n",
    "    \n",
    "    :param directory_path: The path to the directory\n",
    "    \"\"\"\n",
    "    \n",
    "    # Check if the directory exists\n",
    "    if os.path.exists(directory_path):\n",
    "        print(f\"Directory {directory_path} exists. Deleting...\")\n",
    "        \n",
    "        # Delete the directory\n",
    "        shutil.rmtree(directory_path)\n",
    "        \n",
    "        # Create the directory\n",
    "        print(f\"Creating directory {directory_path}...\")\n",
    "        os.makedirs(directory_path)\n",
    "    else:\n",
    "        # If the directory doesn't exist, create it\n",
    "        print(f\"Directory {directory_path} does not exist. Creating...\")\n",
    "        os.makedirs(directory_path)\n",
    "\n",
    "\n",
    "# Specify the directory path\n",
    "\n",
    "\n",
    "parser = argparse.ArgumentParser(description='Process some integers.')\n",
    "parser.add_argument('--model_path', type=str, help='Path of the model to run generation from')\n",
    "parser.add_argument('--num_return_sequences', type=int, help='Number of sequences to generate', default=1000)\n",
    "parser.add_argument('--max_length', type=int, help='Maximum length of generated sequences', default=50)\n",
    "parser.add_argument('--starts_with', type=str, help='Starting amino acids for generation', default='')\n",
    "parser.add_argument('--output_dir', type=str, help='Directory for storing all output files')\n",
    "parser.add_argument('--pred_model_path', type=str, help='Path of the model to predict properties of the sequences')\n",
    "parser.add_argument('--seed', type=int, help='Random seed', default=42)\n",
    "\n",
    "\n",
    "\n",
    "args = parser.parse_args()\n",
    "\n",
    "torch.manual_seed(args.seed)\n",
    "np.random.seed(args.seed)\n",
    "\n",
    "# Create the directory\n",
    "create_or_replace_directory(args.output_dir)\n",
    "protgpt2 = pipeline('text-generation', model=args.model_path, device_map=\"auto\")\n",
    "\n",
    "print('Starting generation...')\n",
    "sequences = []\n",
    "all_amines = ['L','A','G','V','E','S','I','K','R','D','T','P','N','Q','F','Y','M','H','C','W']\n",
    "for cha in all_amines:\n",
    "    sequences_cha = protgpt2(\"<|endoftext|>\" + cha, max_length=args.max_length, do_sample=True, top_k=950, repetition_penalty=1.2, num_return_sequences=args.num_return_sequences//len(all_amines), eos_token_id=0)\n",
    "    sequences.extend(sequences_cha)\n",
    "print('Generation Complete.')\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print('Using Device', device)\n",
    "\n",
    "#Convert the sequence to a string like this\n",
    "#(note we have to introduce new line characters every 60 amino acids,\n",
    "#following the FASTA file format).\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"nferruz/ProtGPT2\")\n",
    "model = GPT2LMHeadModel.from_pretrained(args.model_path)\n",
    "model = model.to(device)\n",
    "\n",
    "# ppl function\n",
    "def calculatePerplexity(sequence, model, tokenizer):\n",
    "    input_ids = torch.tensor(tokenizer.encode(sequence)).unsqueeze(0)\n",
    "    input_ids = input_ids.to(device)\n",
    "    with torch.no_grad():\n",
    "        outputs = model(input_ids, labels=input_ids)\n",
    "    loss, logits = outputs[:2]\n",
    "\n",
    "    return math.exp(loss)\n",
    "\n",
    "\n",
    "print('Starting to order by perplexity...')\n",
    "\n",
    "ppls = []\n",
    "sequences_with_ppl = []\n",
    "\n",
    "for sequence in tqdm(sequences):\n",
    "    seq = sequence['generated_text']\n",
    "    seq = seq.replace(\"<|endoftext|>\", \"\")\n",
    "    seq = seq.replace('\\n', '')\n",
    "    seq = seq.strip()\n",
    "    sequences_with_ppl.append(seq)\n",
    "    seq = '\\n'.join(seq[i:i+60] for i in range(0, len(seq), 60))\n",
    "    seq = '<|endoftext|>' + seq + '<|endoftext|>'\n",
    "    ppl = calculatePerplexity(seq, model, tokenizer)\n",
    "    ppls.append(ppl)\n",
    "\n",
    "#storing all generated proteins with their perplexity values in csv\n",
    "df_ppl = pd.DataFrame()\n",
    "df_ppl['Sequence'] = sequences_with_ppl\n",
    "df_ppl['Perplexity'] = ppls\n",
    "df_ppl.to_csv(args.output_dir + '/all_generated_with_perplexity.csv', index=False)\n",
    "\n",
    "k = args.num_return_sequences//3\n",
    "\n",
    "top_prots = np.array(sequences_with_ppl)[np.argsort(ppls)[:k]]\n",
    "top_prots = [i for i in top_prots if len(i) != 0]\n",
    "print(top_prots)\n",
    "\n",
    "def remove_prots_with(lst, chars_to_exclude):\n",
    "    # Convert the exclusion string to a set for efficient lookup\n",
    "    exclusion_set = set(chars_to_exclude)\n",
    "    \n",
    "    # Use a list comprehension to filter out strings\n",
    "    filtered_list = [s for s in lst if not any(char in exclusion_set for char in s)]\n",
    "    \n",
    "    return filtered_list\n",
    "\n",
    "# Example usage:\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "print('Ordered by Perplexity.')\n",
    "\n",
    "#checking if sequences are inside the hull\n",
    "# Load the .npz file\n",
    "data = np.load('./hull_equations.npz')\n",
    "\n",
    "df_valid = pd.DataFrame()\n",
    "df_valid['Sequence'] = list(top_prots)\n",
    "\n",
    "def create_3d_point(sequence, c):\n",
    "    # Step 1: Count the number of occurrences of character c\n",
    "    normalized_chars = ['X','U','B','Z','O','J']\n",
    "\n",
    "    for norm_char in normalized_chars:\n",
    "        sequence = sequence.replace(norm_char, '')\n",
    "\n",
    "    count_c = sequence.count(c)\n",
    "    \n",
    "    if count_c == 0:\n",
    "        return [0, 0, 0]  # Return None if the character c is not found\n",
    "    \n",
    "    # Step 2: Calculate the mean distance from the start of all occurrences of c\n",
    "    indices_c = [i for i, char in enumerate(sequence) if char == c]\n",
    "    mean_distance = sum(indices_c) / count_c\n",
    "    \n",
    "    # Step 3: Compute the second normalized central moment of the distribution of c\n",
    "    variance = sum((idx - mean_distance) ** 2 for idx in indices_c) / count_c\n",
    "    second_moment = variance / (len(sequence))\n",
    "    \n",
    "    # Create the 3D vector\n",
    "    point = [count_c, mean_distance, second_moment]\n",
    "    \n",
    "    return point\n",
    "\n",
    "def point_in_hull(point, hull_equations, tolerance=1e-12):\n",
    "    return all(\n",
    "        (np.dot(eq[:-1], point) + eq[-1] <= tolerance)\n",
    "        for eq in hull_equations)\n",
    "\n",
    "# Iterate over all arrays inside the file\n",
    "def check_validity(seq):\n",
    "    inside_hulls = []\n",
    "    for c in data.files:\n",
    "        equations = data[c]\n",
    "        point = create_3d_point(seq, c)\n",
    "        inside_hulls.append(point_in_hull(point, equations))\n",
    "    return all(inside_hulls)\n",
    "\n",
    "\n",
    "print('Starting Protein Validity Check...')\n",
    "df_valid['Valid Protein'] = [check_validity(seq) for seq in top_prots]\n",
    "\n",
    "filtered_prots = [prot for prot in top_prots if check_validity(prot)]\n",
    "\n",
    "print('Valid proteins identified.')\n",
    "del model\n",
    "del protgpt2\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "print('Starting structure check...')\n",
    "esm_model = EsmForProteinFolding.from_pretrained(\"facebook/esmfold_v1\")\n",
    "esm_tokenizer = AutoTokenizer.from_pretrained(\"facebook/esmfold_v1\")\n",
    "\n",
    "\n",
    "\n",
    "# Define quantization configuration\n",
    "# quantization_config = QuantoConfig(weights=\"int8\")\n",
    "# esm_model = EsmForProteinFolding.from_pretrained(\"facebook/esmfold_v1\", quantization_config=quantization_config)\n",
    "\n",
    "# Quantize the model\n",
    "\n",
    "\n",
    "esm_model = esm_model.to(device)\n",
    "\n",
    "plddt_results = []\n",
    "\n",
    "\n",
    "for sequence in tqdm(top_prots):\n",
    "    # Input the sequence to ESM model\n",
    "    input_sequence = sequence.replace('\\n', '')\n",
    "    if len(input_sequence) == 0:\n",
    "        continue\n",
    "    inputs = esm_tokenizer([input_sequence], return_tensors=\"pt\", add_special_tokens=False)\n",
    "    inputs = inputs.to(device)\n",
    "    outputs = esm_model(**inputs)\n",
    "    avg_plddt = torch.mean(outputs.plddt)\n",
    "    plddt_results.append(avg_plddt.item())\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "\n",
    "df_valid['plDDT'] = plddt_results\n",
    "plddt_results = np.array(plddt_results)\n",
    "df_valid['Good Structure'] = list(plddt_results > 0.7)\n",
    "\n",
    "#Save valid protein and structure check data to csv\n",
    "df_valid.to_csv(args.output_dir + '/generation_checks.csv', index=False)\n",
    "\n",
    "plddt_results = np.array([plddt_results[i] for i in range(len(top_prots)) if check_validity(top_prots[i])])\n",
    "\n",
    "filtered_prots = np.array(filtered_prots)\n",
    "\n",
    "best_prots = filtered_prots[plddt_results > 0.7]\n",
    "\n",
    "print('Structure check complete. ')\n",
    "\n",
    "\n",
    "\n",
    "del esm_model\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "\n",
    "#Loading PeptideBERT model\n",
    "\n",
    "from PeptideBERT.data.dataloader import load_data\n",
    "from PeptideBERT.model.network import create_model, cri_opt_sch\n",
    "from PeptideBERT.model.utils import train, validate, test\n",
    "\n",
    "\n",
    "save_dir = args.pred_model_path\n",
    "\n",
    "config = yaml.load(open(save_dir + '/config.yaml', 'r'), Loader=yaml.FullLoader)\n",
    "config['device'] = device\n",
    "\n",
    "peptideBERT_model = create_model(config)\n",
    "\n",
    "\n",
    "peptideBERT_model.load_state_dict(torch.load(f'{save_dir}/model.pt')['model_state_dict'], strict=False)\n",
    "\n",
    "\n",
    "m2 = dict(zip(\n",
    "    ['[PAD]','[UNK]','[CLS]','[SEP]','[MASK]','L',\n",
    "    'A','G','V','E','S','I','K','R','D','T','P','N',\n",
    "    'Q','F','Y','M','H','C','W','X','U','B','Z','O'],\n",
    "    range(30)\n",
    "))\n",
    "\n",
    "def f(seq):\n",
    "    seq = map(lambda x: m2[x], seq)\n",
    "    seq = torch.tensor([*seq], dtype=torch.long).unsqueeze(0).to(device)\n",
    "    attn = torch.tensor(seq > 0, dtype=torch.long).to(device)\n",
    "\n",
    "    return seq, attn\n",
    "\n",
    "df_preds = pd.DataFrame()\n",
    "\n",
    "scores = {}\n",
    "\n",
    "print('Starting protein property predictions...')\n",
    "\n",
    "peptides_to_exclude = \"XUBZOJ\"\n",
    "\n",
    "best_prots = remove_prots_with(best_prots, peptides_to_exclude)\n",
    "\n",
    "\n",
    "for seq in tqdm(best_prots):\n",
    "\n",
    "    \n",
    "\n",
    "    seq = seq.replace('\\n', '')\n",
    "\n",
    "\n",
    "\n",
    "    score = peptideBERT_model(*f(seq)).item()\n",
    "\n",
    "    scores[seq] = score\n",
    "\n",
    "print('Property prediction completed.')\n",
    "\n",
    "df_preds['Sequence'] = scores.keys()\n",
    "df_preds['Score'] = scores.values()\n",
    "df_preds['Property'] = df_preds['Score'] > 0.5\n",
    "\n",
    "total_proteins_with_property = np.sum(np.array(list(scores.values())) > 0.5)\n",
    "\n",
    "property_probability = total_proteins_with_property/len(best_prots)\n",
    "\n",
    "# Save Predictions to CSV\n",
    "df_preds.to_csv(args.output_dir + '/predictions.csv', index=False)\n",
    "\n",
    "print('Inference Complete')\n",
    "\n",
    "with open(args.output_dir + '/info.txt', 'w') as file:\n",
    "    # Write 'print('hello world')' to the file\n",
    "    file.write(f'Total generated sequences {args.num_return_sequences} out of which top {args.num_return_sequences//3} sequences based on perplexity were chosen.\\n')\n",
    "    file.write(f'Total sequences which passed protein validty check {len(filtered_prots)}, rejected {len(top_prots) - len(filtered_prots)} or {((len(top_prots) - len(filtered_prots))/len(top_prots) * 100):.3f}%\\n')\n",
    "    file.write(f'Total proteins which had plDDT > 0.7 from ESMFold are {len(best_prots)}, {(len(best_prots)/len(filtered_prots)*100):.3f}% of valid generated proteins\\n')\n",
    "    file.write(f'{total_proteins_with_property}/{len(best_prots)} had the desired property, which is {property_probability:.4f} probability.\\n')\n",
    "    for arg in vars(args):\n",
    "        file.write(f\"{arg}: {getattr(args, arg)}\\n\")\n",
    "\n",
    "print(f'Total generated sequences {args.num_return_sequences} out of which top {args.num_return_sequences//3} sequences based on perplexity were chosen.')\n",
    "print(f'Total sequences which passed protein validty check {len(filtered_prots)}, rejected {len(top_prots) - len(filtered_prots)} or {((len(top_prots) - len(filtered_prots))/len(top_prots) * 100):.3f}%')\n",
    "print(f'Total proteins which had plDDT > 0.7 from ESMFold are {len(best_prots)}, {(len(best_prots)/len(filtered_prots) * 100):.3f}% of valid generated proteins')\n",
    "print(f'{total_proteins_with_property}/{len(best_prots)} had the desired property, which is {property_probability:.4f} probability.')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "BIA",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
